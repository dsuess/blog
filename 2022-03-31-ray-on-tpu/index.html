<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Pytorch Hyperparameter Optimization on TPUs | Matmuls all the way down</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Pytorch Hyperparameter Optimization on TPUs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="So, you finally got access to those sweet free TPUs through Googleâ€™s TRC program and want to make good use of it? Then youâ€™ve come to the right place. In this post, we will go over how to efficiently distribute hyperparameter optimization of a Pytorch model across multiple TPUs using Ray tune. More specifically, we will tune a Pytorch model from the fantastic ðŸ¤—-transformer library." />
<meta property="og:description" content="So, you finally got access to those sweet free TPUs through Googleâ€™s TRC program and want to make good use of it? Then youâ€™ve come to the right place. In this post, we will go over how to efficiently distribute hyperparameter optimization of a Pytorch model across multiple TPUs using Ray tune. More specifically, we will tune a Pytorch model from the fantastic ðŸ¤—-transformer library." />
<link rel="canonical" href="https://dsuess.github.io/blog/2022-03-31-ray-on-tpu/" />
<meta property="og:url" content="https://dsuess.github.io/blog/2022-03-31-ray-on-tpu/" />
<meta property="og:site_name" content="Matmuls all the way down" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-31T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://dsuess.github.io/blog/2022-03-31-ray-on-tpu/","@type":"BlogPosting","headline":"Pytorch Hyperparameter Optimization on TPUs","dateModified":"2022-03-31T00:00:00-05:00","datePublished":"2022-03-31T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dsuess.github.io/blog/2022-03-31-ray-on-tpu/"},"description":"So, you finally got access to those sweet free TPUs through Googleâ€™s TRC program and want to make good use of it? Then youâ€™ve come to the right place. In this post, we will go over how to efficiently distribute hyperparameter optimization of a Pytorch model across multiple TPUs using Ray tune. More specifically, we will tune a Pytorch model from the fantastic ðŸ¤—-transformer library.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dsuess.github.io/blog/feed.xml" title="Matmuls all the way down" /><link rel="shortcut icon" type="image/x-icon" href="/blog/assets/img/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Matmuls all the way down</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/_pages/about/">About</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Pytorch Hyperparameter Optimization on TPUs</h1><h2 class="post-subtitle p-name">Making Pytorch-XLA play nice with Ray Tune</h2><p class="post-meta post-meta-title">Posted on
      <time class="dt-published" datetime="2022-03-31T00:00:00-05:00" itemprop="datePublished">
        Mar 31, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i>
      
        <a class="category-tags-link" href="/blog/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#distributed">distributed</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#hardware">hardware</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#TPU">TPU</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>So, you finally got access to those sweet free TPUs through Googleâ€™s <a href="https://sites.research.google/trc/about/">TRC program</a> and want to make good use of it?
Then youâ€™ve come to the right place.
In this post, we will go over how to efficiently distribute hyperparameter optimization of a Pytorch model across multiple TPUs using Ray tune.
More specifically, we will tune a Pytorch model from the fantastic ðŸ¤—-transformer library.</p>

<blockquote>
  <p>Thanks to Googleâ€™s TPU Research Cloud program for providing free access to cutting-edge TPUs. For the full code, go to <a href="https://github.com/dsuess/transformers-mup/tree/ray-on-tpu">https://github.com/dsuess/transformers-mup/tree/ray-on-tpu</a>.</p>
</blockquote>

<h2 id="introduction">Introduction</h2>

<p>Originally, this post started out with me wanting to try <a href="https://github.com/microsoft/mup">Maximal Update Parmemetrization</a> (Î¼P) on ðŸ¤—-transformers.
Î¼P are a special parameterization of trainable parameters in Deep Neural Networks.
Models in Î¼P have the nice property that their optimal hyperparameters are stable across different network sizes:</p>

<p><img src="../images/2022-03-31-ray-on-tpu_hyperparams.png" alt="" title="Training loss as a function of learning rate for different model sizes with a standard parameterization (left) and maximal update parametrization (right). Taken from https://github.com/microsoft/mup." /></p>

<p>In other words, we can use a small model-variant to find hyperparameters, which are nearly optimal for all model sizes.
As a result, we do not have to perform hyperparameter optimization separately on the larger model-variants, and hence, we can train large models nearly optimally for a fraction of the compute cost.</p>

<p>The goal was to run these experiments using TPUs from Googleâ€™s TPU Research Cloud program, which provides free access to multiple v2-8 and v3-8 TPUs.
These are single-node instances with 8 TPU cores each, which makes them fast enough to train smaller model variants.</p>

<p>The good news is that ðŸ¤—-transformers comes with support for hyperparameter optimization <a href="https://huggingface.co/blog/ray-tune">out of the box</a>.
Using Ray tune, we can easily scale the hyperparameter search across many nodes when using GPUs.
For reasons that we will outline <a href="#p-for-power">below</a>, out-of-the-box support for TPUs in Ray is currently limited:
We can either run on multiple nodes, but with the limit of only utilizing a single TPU-core per node.
Alternatively, if we want to use all 8 TPU cores on a node, we are limited to a single instance</p>

<p>This post explains how to get around this limitation and unlock the full power of TPUs for hyperparameter optimization in Ray tune when using Pytorch.</p>

<h2 id="hyperparamer-search">Hyperparamer Search</h2>

<p>Hyperparameters are parameters of a deep learning model that are not tuned by gradient descent, e.g. learning rate, weight decay, or the number of layers in the architecture.</p>

<p>The standard approach to pick a good set of hyperparameters is through <em>hyperparameter optimization</em>, which requires training multiple models, each with a different set of hyperparameters, in order to select the one with the best performance.
On the one hand, the large number of trials required makes hyperparameter optimization computationally costly.
On the other hand, multiple trials can be evaluated in parallel without the need of communication while training.
This makes it feasible, and often necessary, to run the workload on multiple nodes in parallel.</p>

<p>This is where <a href="https://docs.ray.io/en/latest/tune/index.html">Ray Tune</a> shines:
It is a library for distributed hyperparameter optimization build on top of Ray.
With only a few lines of code, we can can implement state-of-the-art hyperparameter optimization techniques.
All we need to do is define the search space of hyperparameters and a function that registers the metrics for our model (see the <a href="https://github.com/dsuess/transformers-mup/blob/ray-on-tpu/run.py#L77">github repo</a> for the actual implementation):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">ray.tune</span> <span class="k">as</span> <span class="n">rt</span>

<span class="k">def</span> <span class="nf">run_train</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
    <span class="p">...</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">config</span><span class="p">[</span><span class="s">"num_train_epochs"</span><span class="p">]:</span>
        <span class="p">...</span>
        <span class="n">rt</span><span class="p">.</span><span class="n">report</span><span class="p">(</span><span class="n">eval_loss</span><span class="o">=</span><span class="p">...)</span>

<span class="n">search_space</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"learning_rate"</span><span class="p">:</span> <span class="n">rt</span><span class="p">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">),</span>
    <span class="s">"warmup_ratio"</span><span class="p">:</span> <span class="n">rt</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="s">"num_train_epochs"</span><span class="p">:</span> <span class="n">rt</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">train_fun</span> <span class="o">=</span> <span class="n">ft</span><span class="p">.</span><span class="n">partial</span><span class="p">(</span><span class="n">run_train</span><span class="p">,</span> <span class="p">...)</span>
<span class="n">analysis</span> <span class="o">=</span> <span class="n">rt</span><span class="p">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">train_fun</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">search_space</span><span class="p">,</span>
    <span class="n">time_budget_s</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span><span class="p">,</span>
    <span class="n">search_alg</span><span class="o">=</span><span class="n">HyperOptSearch</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s">"eval_loss"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"min"</span><span class="p">),</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">ASHAScheduler</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s">"eval_loss"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"min"</span><span class="p">),</span>
    <span class="p">...</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The same code can now run either on your local machine or on a <a href="https://docs.ray.io/en/latest/cluster/index.html">Ray cluster</a>.
For example, the <a href="https://github.com/dsuess/transformers-mup/blob/ray-on-tpu/cluster.yaml">cluster</a> we will run on consists of a light-weight head node to manage the cluster and three TPU v2-8 nodes.
If it was not for the limited availability of TPU nodes, we could easily scale this implementation to few 10s or 100s of workers.</p>

<h2 id="p-for-power">P for Power</h2>

<p>The other key in making hyperparameter search for deep learning models fast is to distribute the training in each trial across multiple accelerators (i.e. GPUs or TPUs).
For multi-GPU training, Ray tune nicely integrates with the both pytorchâ€™s <a href="https://docs.ray.io/en/latest/tune/tutorials/tune-pytorch-cifar.html#adding-multi-gpu-support-with-dataparallel">DataParallel</a> and <a href="https://docs.ray.io/en/latest/tune/tutorials/tune-pytorch-cifar.html#advanced-distributed-training-with-distributeddataparallel">DistributedDataParallel</a>.
The latter can be significantly faster, even for single-node training, as it runs a separate process for each GPU.</p>

<p>The same is true when running on TPU nodes:
Each node has 8 separate TPU cores and each core requires a seprate Python process to run.
However, launching the necessary processes requires additional bookkeeping and should be done directly through <a href="https://github.com/pytorch/xla">PyTorch XLA</a>, the library that enables using TPUs in PyTorch.
Therefore, we cannot simply re-use the existing DistributedDataParallel wrapper provided by Ray tune.</p>

<p>Instead, we will manually need to launch the TPU-core worker processes on each Ray worker node using Pytorch XLAâ€™s multiprocessing module:</p>

<p><img src="../images/2022-03-31-overview.svg" alt="" title="Overview of the processes being launched. The blue processes are managed by the Ray runtime, whereras the orange processes are managed manually through Pytorch XLA." /></p>

<p>The remaining challenge is how to return the modelâ€™s validation scores back to Ray tune.
Ideally, we report the scores back after each epoch.
This way we can make use of early stopping and terminate unfavourable trials early on.
Since the scores are computed in the TPU-worker processes, but reporting to Ray-tune needs to happen from the Ray-worker process, we use a simple Queue to push results from the former to the latter (<a href="https://github.com/dsuess/transformers-mup/blob/ray-on-tpu/run.py#L22">full implementation</a>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_handle_distributed</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
    <span class="kn">import</span> <span class="nn">torch_xla.distributed.xla_multiprocessing</span> <span class="k">as</span> <span class="n">xmp</span>

    <span class="n">mp</span><span class="p">.</span><span class="n">set_start_method</span><span class="p">(</span><span class="s">"spawn"</span><span class="p">)</span>
    <span class="n">queue</span> <span class="o">=</span> <span class="n">mp</span><span class="p">.</span><span class="n">Queue</span><span class="p">()</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">xmp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">_launch_mp</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">queue</span><span class="p">),</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>Ray-tune launches this function on each Ray-worker node with different hyperparameter-values in <code class="language-plaintext highlighter-rouge">config</code>.
Then in turn, the last line launches 8 worker processes on each node â€“ one for each TPU core â€“ with the entrypoint <a href="https://github.com/dsuess/transformers-mup/blob/ray-on-tpu/training.py#L30"><code class="language-plaintext highlighter-rouge">_launch_mp</code></a>, which contains the whole training logic.
We set <code class="language-plaintext highlighter-rouge">join=False</code> so the Ray-worker node can continue running and process messages sent by the TPU-workers through <code class="language-plaintext highlighter-rouge">queue</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">try</span><span class="p">:</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">is_alive</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">context</span><span class="p">.</span><span class="n">processes</span><span class="p">):</span>
                <span class="k">break</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">queue</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">Empty</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">rt</span><span class="p">.</span><span class="n">report</span><span class="p">(</span><span class="o">**</span><span class="n">res</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">(</span><span class="s">f"No idea how to handle </span><span class="si">{</span><span class="n">res</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="n">queue</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
        <span class="n">context</span><span class="p">.</span><span class="n">join</span><span class="p">()</span>
</code></pre></div></div>

<p>As already mentioned above, the function <code class="language-plaintext highlighter-rouge">_launch_mp</code> contains the whole training logic, which in this case is a slightly modified <a href="https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb">ðŸ¤—-transformers example</a><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_launch_mp</span><span class="p">(</span>
    <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">num_cores</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">result_queue</span><span class="p">:</span> <span class="n">Queue</span> <span class="o">=</span> <span class="bp">None</span>
<span class="p">):</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"sgugger/gpt2-like-tokenizer"</span><span class="p">)</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">model_max_length</span>

    <span class="k">def</span> <span class="nf">tokenize_fn</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"text"</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">group_texts</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
        <span class="n">concatenated</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="p">[])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">}</span>
        <span class="n">key</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">examples</span><span class="p">.</span><span class="n">keys</span><span class="p">()</span>
        <span class="n">total</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">concatenated</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">concatenated</span><span class="p">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="n">result</span><span class="p">[</span><span class="s">"labels"</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s">"input_ids"</span><span class="p">].</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="n">model_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s">"gpt2"</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span>

    <span class="n">lm_datasets</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">load_dataset</span><span class="p">(</span><span class="s">"wikitext"</span><span class="p">,</span> <span class="s">"wikitext-2-raw-v1"</span><span class="p">)</span>
        <span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_proc</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s">"text"</span><span class="p">])</span>
        <span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">group_texts</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_proc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="s">f"gpt2-wikitext2"</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s">"epoch"</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"learning_rate"</span><span class="p">,</span> <span class="mf">2e-5</span><span class="p">),</span>
        <span class="n">warmup_ratio</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"warmup_ratio"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"weight_decay"</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"num_train_epochs"</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"gradient_accumulation_steps"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">eval_accumulation_steps</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"gradient_accumulation_steps"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="n">push_to_hub</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">save_strategy</span><span class="o">=</span><span class="s">"no"</span><span class="p">,</span>
        <span class="n">tpu_num_cores</span><span class="o">=</span><span class="n">num_cores</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">callbacks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TrainerCallback</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">result_queue</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">callbacks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">TuneReporterCallback</span><span class="p">(</span><span class="n">result_queue</span><span class="p">))</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="o">=</span><span class="n">lm_datasets</span><span class="p">[</span><span class="s">"train"</span><span class="p">],</span>
        <span class="n">eval_dataset</span><span class="o">=</span><span class="n">lm_datasets</span><span class="p">[</span><span class="s">"validation"</span><span class="p">],</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">trainer</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div></div>

<p>The main diffference here is that we set the <code class="language-plaintext highlighter-rouge">tpu_num_cores</code> argument in <code class="language-plaintext highlighter-rouge">TrainerArguments</code> and register a custom <code class="language-plaintext highlighter-rouge">TuneReporterCallback</code>.
The latter uses the result queue shared between the parent and all workers processes to report the results of evaluation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers.trainer_callback</span> <span class="kn">import</span> <span class="n">TrainerCallback</span><span class="p">,</span> <span class="n">TrainerState</span>

<span class="k">class</span> <span class="nc">TuneReporterCallback</span><span class="p">(</span><span class="n">TrainerCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result_queue</span><span class="p">:</span> <span class="n">Queue</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">result_queue</span> <span class="o">=</span> <span class="n">result_queue</span>

    <span class="k">def</span> <span class="nf">on_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">TrainerState</span><span class="p">,</span> <span class="n">control</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">result_queue</span><span class="p">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">log_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>To make sure we do not record metrics multiple times, the callback is only registered on the 0th TPU worker node with <code class="language-plaintext highlighter-rouge">idx==0</code>.</p>

<h2 id="running">Running</h2>

<p>The code can run both on a single TPU VM or distributed on a Ray cluster.
For the former, simply start a TPU VM with the <code class="language-plaintext highlighter-rouge">tpu-vm-pt-1.10</code> base image, install the dependencies in <code class="language-plaintext highlighter-rouge">requirements.txt</code>, and run</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python3 run.py tune
</code></pre></div></div>
<p>To run it on an existing Ray cluster, simply pass its IP via the <code class="language-plaintext highlighter-rouge">--address</code> parameter<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup>.
Using the provided <a href="https://github.com/dsuess/transformers-mup/blob/ray-on-tpu/cluster.yaml">cluster configuration</a> with three worker nodes, the output of the <code class="language-plaintext highlighter-rouge">run.py</code> script will look something like this after a while:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of trials: 9/infinite (1 PENDING, 3 RUNNING, 5 TERMINATED)
+------------------+------------+----------------------+-----------------+--------+-------------+
| Trial name       | status     | loc                  |   learning_rate |   iter |   eval_loss |
|------------------+------------+----------------------+-----------------+--------+-------------+
| DEFAULT_700ebfd4 | RUNNING    | 10.128.15.216:295419 |     0.001869    |     19 |     5.73066 |
| DEFAULT_92adfe30 | RUNNING    | 10.128.15.210:496115 |     0.00258847  |      1 |     7.1485  |
| DEFAULT_0f17d982 | RUNNING    | 10.128.15.217:378231 |     0.000924847 |        |             |
| DEFAULT_21656a3c | PENDING    |                      |     1.52227e-05 |        |             |
| DEFAULT_4ddb65bc | TERMINATED | 10.128.15.210:7846   |     3.73523e-06 |     43 |     7.2199  |
| DEFAULT_4eb638e0 | TERMINATED | 10.128.15.216:8352   |     7.79511e-06 |     30 |     7.09317 |
| DEFAULT_060b8f5e | TERMINATED | 10.128.15.217:7332   |     0.000317765 |     40 |     5.20176 |
| DEFAULT_c231fb58 | TERMINATED | 10.128.15.210:396501 |     4.85264e-05 |      1 |     9.42292 |
| DEFAULT_0f59cdfc | TERMINATED | 10.128.15.210:446377 |     4.47327e-05 |      1 |     9.45109 |
+------------------+------------+----------------------+-----------------+--------+-------------+
</code></pre></div></div>

<p>The first three lines show the running trials, which demonstrates that all three workers are properly utilized.
Additionally, the trials in the last two lines were terminated by Ray tuneâ€™s early stopping algorithm, which shows that the TPU workers can be killed safely.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This post went over the steps necessary for getting pytorchâ€™s TPU support to work seamlessly in Ray tune.
We are now able to run hyperparameter optimization in paralllel on multiple TPU nodes while also making full use of the accelerators.
Since Ray does not support TPU pods â€“ collections of multiple TPU nodes for distributed multi-node training â€“ we are currently restricted to v2-8 and v3-8 TPU instances, and hence, smaller models.
Nevertheless, with Ray tune, we can easily run the trials for hyperparameter optimization on as many TPU nodes as we have access to (or as many as we are willing to pay for).</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>To use this code with any custom Pytorch model, only the <em>_lauch_mp</em> function would need to be modified.Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>For full instructions see the <a href="https://github.com/dsuess/transformers-mup/blob/ray-on-tpu/README.md">readme</a>. If you are enrolled into TRC, make sure to adapt the region and project settings in <em>cluster.yaml</em> to use TPUs from the free quota. Running multiple TPU nodes can become expensive very quickly.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/2022-03-31-ray-on-tpu/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dsuess" target="_blank" title="dsuess"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/daniel-suess" target="_blank" title="daniel-suess"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/suess_daniel" target="_blank" title="suess_daniel"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
