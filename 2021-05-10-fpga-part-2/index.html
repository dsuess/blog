<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>From Scratchː Neural Network Inference on FPGAs – Part 2 | Matmuls all the way down</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="From Scratchː Neural Network Inference on FPGAs – Part 2" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Now that the last post covered how to build the example for emulation and hardware deployment, we are now ready to go in a bit more detail. This post will go over this simple baseline implementation for a 2 layer fully connected network running inference on FPGAs." />
<meta property="og:description" content="Now that the last post covered how to build the example for emulation and hardware deployment, we are now ready to go in a bit more detail. This post will go over this simple baseline implementation for a 2 layer fully connected network running inference on FPGAs." />
<link rel="canonical" href="https://dsuess.github.io/blog/2021-05-10-fpga-part-2/" />
<meta property="og:url" content="https://dsuess.github.io/blog/2021-05-10-fpga-part-2/" />
<meta property="og:site_name" content="Matmuls all the way down" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-10T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://dsuess.github.io/blog/2021-05-10-fpga-part-2/","@type":"BlogPosting","headline":"From Scratchː Neural Network Inference on FPGAs – Part 2","dateModified":"2021-05-10T00:00:00-05:00","datePublished":"2021-05-10T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dsuess.github.io/blog/2021-05-10-fpga-part-2/"},"description":"Now that the last post covered how to build the example for emulation and hardware deployment, we are now ready to go in a bit more detail. This post will go over this simple baseline implementation for a 2 layer fully connected network running inference on FPGAs.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://dsuess.github.io/blog/feed.xml" title="Matmuls all the way down" /><link rel="shortcut icon" type="image/x-icon" href="/blog/assets/img/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Matmuls all the way down</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/_pages/about/">About</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">From Scratchː Neural Network Inference on FPGAs – Part 2</h1><h2 class="post-subtitle p-name">A baseline implementation for fully connected networks</h2><p class="post-meta post-meta-title">Posted on
      <time class="dt-published" datetime="2021-05-10T00:00:00-05:00" itemprop="datePublished">
        May 10, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i>
      
        <a class="category-tags-link" href="/blog/categories/#fpga">fpga</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#hardware">hardware</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Now that the last post covered how to build the <a href="https://github.com/dsuess/nn-on-fpgas/tree/part-1-v1">example</a> for emulation and hardware deployment, we are now ready to go in a bit more detail.
This post will go over this simple baseline implementation for a 2 layer fully connected network running inference on FPGAs.</p>

<h2 id="contents">Contents</h2>

<blockquote>
  <p><a href="/blog/2021-05-09-fpga-part-1/"><strong>Part 1</strong>: How to build FPGA applications on AWS</a> <br />
<a href="/blog/2021-05-10-fpga-part-2/"><strong>Part 2</strong>: A baseline implementation for fully connected networks (this post)</a></p>
</blockquote>

<h2 id="code-walkthrough">Code walkthrough</h2>

<p>The <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/train.py">model</a> we’re trying to run is as simple as it gets.
Two fully connected layers; the first one ReLu-activated and the second (and final) layer with a softmax activation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FCNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">relu6</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">logsoftmax</span><span class="p">()</span>
</code></pre></div></div>
<p>This means we’ll need to implement the following three kernels:</p>
<ul>
  <li>matrix-matrix multiplication</li>
  <li>bias-addition + ReLu non-linearity</li>
  <li>bias-addition + softmax non-linearity</li>
</ul>

<p>The reason for separating the matrix-multiplication and bias-addition operations into two kernels is that the former is likely going to be the biggest performance bottleneck.
Therefore, keeping them separated will make experimentation with different matrix-multiplication implementation easier later on.<br />
Also, fusing the bias-addition and non-linearity functions into a single kernel will likely be beneficial for performance.
Both operations are relatively fast, so the additional overhead from launching a kernel will be more noticeable.
Both operations also have a low computation-to-memory-transfer ratio, and hence, benefit from reducing the memory access by executing both bias-addition and non-linearity in one pass.</p>

<p>The structure of the model is also duplicated for the C++ implementation in the <a href="https://github.com/dsuess/nn-on-fpgas/blob/f301d3053bbc0ab7baea4ea92b440add63734369/src/net.hpp"><code class="language-plaintext highlighter-rouge">net.hpp</code></a> file, which we will cover later.</p>

<h2 id="kernel-implementations">Kernel implementations</h2>

<p>The first and simplest kernel we’ll write is the bias + ReLu6 kernel.
Thanks to the HLS using C++ and not including any optional, FPGA-specific code, the first <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/bias_relu6_kernel.cpp">baseline implementation</a> looks like standard C++ code</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">extern</span> <span class="s">"C"</span> <span class="kt">void</span> <span class="nf">bias_relu6_kernel</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="k">const</span> <span class="n">activation</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="k">const</span> <span class="n">bias</span><span class="p">,</span> <span class="k">const</span> <span class="n">uint</span> <span class="n">batch_size</span><span class="p">,</span> <span class="k">const</span> <span class="n">uint</span> <span class="n">dim</span><span class="p">)</span>
<span class="p">{</span>
   <span class="k">for</span> <span class="p">(</span><span class="n">uint</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">;</span> <span class="n">b</span><span class="o">++</span><span class="p">)</span>
   <span class="p">{</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">uint</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">d</span><span class="o">++</span><span class="p">)</span>
      <span class="p">{</span>
         <span class="k">const</span> <span class="n">uint</span> <span class="n">ia</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">b</span> <span class="o">+</span> <span class="n">d</span><span class="p">;</span>
         <span class="n">activation</span><span class="p">[</span><span class="n">ia</span><span class="p">]</span> <span class="o">=</span> <span class="n">relu6</span><span class="p">(</span><span class="n">activation</span><span class="p">[</span><span class="n">ia</span><span class="p">]</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[</span><span class="n">d</span><span class="p">]);</span>
      <span class="p">}</span>
   <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The implementation for the bias + softmax <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/bias_softmax_kernel.cpp">kernel</a> is very similar.
Note that the result is computed inplace using <code class="language-plaintext highlighter-rouge">activation</code> both as an input and an output.
This is not possible for the matrix-multiplication kernel since the shape of either input and the output can be different.
Hence, we need to allocate sufficient memory outside of the kernel and pass in a pointer using the additional <code class="language-plaintext highlighter-rouge">out</code> argument.
The naive implementation using three for-loops is certainly not optimal and will need to be revisited later:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">extern</span> <span class="s">"C"</span> <span class="kt">void</span> <span class="nf">matmul_kernel</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="k">const</span> <span class="n">matrixA</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="k">const</span> <span class="n">matrixB</span><span class="p">,</span> <span class="k">const</span> <span class="n">uint</span> <span class="n">rowsA</span><span class="p">,</span> <span class="k">const</span> <span class="n">uint</span> <span class="n">colsA</span><span class="p">,</span> <span class="k">const</span> <span class="n">uint</span> <span class="n">colsB</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="k">const</span> <span class="n">out</span><span class="p">)</span>
<span class="p">{</span>
   <span class="k">for</span> <span class="p">(</span><span class="n">uint</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">rowsA</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
   <span class="p">{</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">uint</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">colsB</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span>
      <span class="p">{</span>
         <span class="c1">// Nulling result here causes issues when running in hw-emu mode.</span>
         <span class="c1">// Looks like io isn't updated "in time"</span>
         <span class="k">const</span> <span class="n">uint</span> <span class="n">io</span> <span class="o">=</span> <span class="n">colsB</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
         <span class="k">for</span> <span class="p">(</span><span class="n">uint</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">colsA</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span>
         <span class="p">{</span>
            <span class="k">const</span> <span class="n">uint</span> <span class="n">ia</span> <span class="o">=</span> <span class="n">colsA</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="n">k</span><span class="p">;</span>
            <span class="k">const</span> <span class="n">uint</span> <span class="n">ib</span> <span class="o">=</span> <span class="n">colsB</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
            <span class="n">out</span><span class="p">[</span><span class="n">io</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrixA</span><span class="p">[</span><span class="n">ia</span><span class="p">]</span> <span class="o">*</span> <span class="n">matrixB</span><span class="p">[</span><span class="n">ib</span><span class="p">];</span>
         <span class="p">}</span>
      <span class="p">}</span>
   <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<p>The output array <code class="language-plaintext highlighter-rouge">out</code> should be initialized to zero outside of the kernel and all arrays are assumed to store their elements in row-major order.</p>

<h2 id="host-application">Host application</h2>

<p>So far, we’ve implemented all the code that will “run” on the FPGA device.
The rest, which makes up the majority of the code, is for the host-device (CPU) and needed for memory management as well as dispatching the kernels.</p>

<p>The <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/matrix.hpp#L37"><code class="language-plaintext highlighter-rouge">Matrix</code> class</a> abstracts away the memory management as well as the host-device memory transfers.
One major constraint for using Vitis is that all memory copied to or from the FPGA device needs to be page-aligned on the host device, i.e. both the starting address and memory size have to be divisible by page size.
For this, we use a <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/matrix.hpp#L27">custom allocator</a> with <code class="language-plaintext highlighter-rouge">DEFAULT_ALIGNMENT</code> being hard coded to the page size <code class="language-plaintext highlighter-rouge">4096</code>:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">T</span><span class="p">&gt;</span>
<span class="n">T</span> <span class="o">*</span><span class="nf">aligned_alloc</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">num</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="kt">size_t</span> <span class="n">alignment</span> <span class="o">=</span> <span class="n">DEFAULT_ALIGNMENT</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">void</span> <span class="o">*</span><span class="n">ptr</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">posix_memalign</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span> <span class="n">alignment</span><span class="p">,</span> <span class="n">num</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">)))</span>
    <span class="p">{</span>
        <span class="k">throw</span> <span class="n">std</span><span class="o">::</span><span class="n">bad_alloc</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">T</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The functions <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/matrix.hpp#L178"><code class="language-plaintext highlighter-rouge">to_device</code></a> and <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/matrix.hpp#L196"><code class="language-plaintext highlighter-rouge">to_cpu</code></a> handle memory transfer between the host and the device.
We implement move-semantics for the <code class="language-plaintext highlighter-rouge">Matrix</code> class by implementing special constructors and the <code class="language-plaintext highlighter-rouge">=</code>-operator to allow for copy-free return values from functions.<br />
Finally, the two helper functions at the bottom of <a href="https://github.com/dsuess/nn-on-fpgas/blob/4811b23d4fd4300c53a2f7638d5b9deee93a051d/src/matrix.hpp#L210"><code class="language-plaintext highlighter-rouge">matrix.hpp</code></a> abstract the OpenCL kernel-dispatch overhead for us.
For example, the <code class="language-plaintext highlighter-rouge">apply_matmul</code> function applies the <code class="language-plaintext highlighter-rouge">matmul_kernel</code> to two <code class="language-plaintext highlighter-rouge">Matrix</code> instances:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">Matrix</span><span class="p">,</span> <span class="n">cl</span><span class="o">::</span><span class="n">Event</span><span class="o">&gt;</span> <span class="n">apply_matmul</span><span class="p">(</span><span class="n">Matrix</span> <span class="o">&amp;</span><span class="n">matrixA</span><span class="p">,</span> <span class="n">Matrix</span> <span class="o">&amp;</span><span class="n">matrixB</span><span class="p">,</span> <span class="n">cl</span><span class="o">::</span><span class="n">Kernel</span> <span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cl</span><span class="o">::</span><span class="n">Event</span><span class="o">&gt;</span> <span class="o">*</span><span class="n">wait_on</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">DeviceHandle</span> <span class="o">&amp;</span><span class="n">handle</span> <span class="o">=</span> <span class="n">HANDLE</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">Matrix</span> <span class="n">result</span> <span class="o">=</span> <span class="n">Matrix</span><span class="o">::</span><span class="n">constant</span><span class="p">(</span><span class="n">matrixA</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span> <span class="n">matrixB</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">4096</span><span class="p">);</span>
    <span class="n">result</span><span class="p">.</span><span class="n">to_device</span><span class="p">(</span><span class="n">handle</span><span class="p">);</span>
    <span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">matrixA</span><span class="p">.</span><span class="n">get_buffer</span><span class="p">());</span>
    <span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">matrixB</span><span class="p">.</span><span class="n">get_buffer</span><span class="p">());</span>
    <span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">matrixA</span><span class="p">.</span><span class="n">rows</span><span class="p">);</span>
    <span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">matrixA</span><span class="p">.</span><span class="n">cols</span><span class="p">);</span>
    <span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">matrixB</span><span class="p">.</span><span class="n">cols</span><span class="p">);</span>
    <span class="n">kernel</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">get_buffer</span><span class="p">());</span>

    <span class="n">cl</span><span class="o">::</span><span class="n">Event</span> <span class="n">event</span><span class="p">;</span>
    <span class="n">handle</span><span class="p">.</span><span class="n">q</span><span class="p">.</span><span class="n">enqueueTask</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">wait_on</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">event</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">result</span><span class="p">),</span> <span class="n">event</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Since OpenCL provides an async API, we do not simply invoke the kernel, but we enqueue the task of running the kernel in a <a href="https://livebook.manning.com/book/opencl-in-action/chapter-7/">command queue</a>.
This task may depend on other previously enqueued tasks, which can be expressed using the (optional) <code class="language-plaintext highlighter-rouge">wait_on</code> argument.
A reference to the newly invoked task is returned as the second return value of type <code class="language-plaintext highlighter-rouge">cl::Event</code>.
Only after this task is processed does the return value <code class="language-plaintext highlighter-rouge">result</code> contain the computed value.
To see how this is used, take a look at the <a href="https://github.com/dsuess/nn-on-fpgas/blob/f301d3053bbc0ab7baea4ea92b440add63734369/src/net.hpp#L40">forward-pass</a> of the network:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">Matrix</span> <span class="nf">operator</span><span class="p">()(</span><span class="n">Matrix</span> <span class="o">&amp;</span><span class="n">input</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cl</span><span class="o">::</span><span class="n">Event</span><span class="o">&gt;</span> <span class="n">events</span><span class="p">;</span>
        <span class="n">events</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
        <span class="n">Matrix</span> <span class="n">y</span><span class="p">;</span>
        <span class="n">std</span><span class="o">::</span><span class="n">tie</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">events</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">=</span> <span class="n">apply_matmul</span><span class="p">(</span><span class="n">input</span><span class="p">,</span> <span class="n">weight1</span><span class="p">,</span> <span class="n">MATMUL_KERNEL</span><span class="p">);</span>
</code></pre></div></div>

<p>The matrix-multiplication in the first layer is independent of any operation.
Therefore, we do not pass the <code class="language-plaintext highlighter-rouge">wait_on</code> argument in this line.
The event-result is then assigned to the first entry of the <code class="language-plaintext highlighter-rouge">events</code> vector and duplicated in the following lines to make sure every entry of <code class="language-plaintext highlighter-rouge">events</code> is a valid <code class="language-plaintext highlighter-rouge">cl::Event</code> instance:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">events</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">events</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
        <span class="n">events</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">events</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
        <span class="n">events</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">apply_bias</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bias1</span><span class="p">,</span> <span class="n">BIAS_RELU6_KERNEL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">events</span><span class="p">);</span>
</code></pre></div></div>

<p>A pointer to this vector is then passed on to the bias-activation part of the first layer since it depends on the prior matrix-multiplication to finish.
A similar effect can be achieved by only allocating a vector of size one at this point, but then we would have to resize the vector after each additional operation.
The following lines then apply the second layer of the network accordingly.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        <span class="n">std</span><span class="o">::</span><span class="n">tie</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">events</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">=</span> <span class="n">apply_matmul</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">weight2</span><span class="p">,</span> <span class="n">MATMUL_KERNEL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">events</span><span class="p">);</span>
        <span class="n">apply_bias</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bias2</span><span class="p">,</span> <span class="n">BIAS_SOFTMAX_KERNEL</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">events</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">;</span>
    <span class="err">}</span>
</code></pre></div></div>

<p>Now we are ready to look at the high-level implementation in <a href="https://github.com/dsuess/nn-on-fpgas/blob/f301d3053bbc0ab7baea4ea92b440add63734369/src/main.cpp">main.cpp</a>:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
    <span class="n">init_kernels</span><span class="p">();</span>

    <span class="k">auto</span> <span class="n">model</span> <span class="o">=</span> <span class="n">FCNN</span><span class="p">(</span><span class="s">"weights/"</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">input</span> <span class="o">=</span> <span class="n">Matrix</span><span class="o">::</span><span class="n">from_npy</span><span class="p">(</span><span class="s">"weights/samples.npy"</span><span class="p">);</span>
    <span class="n">input</span><span class="p">.</span><span class="n">to_device</span><span class="p">();</span>
</code></pre></div></div>
<p>The first call to <a href="https://github.com/dsuess/nn-on-fpgas/blob/f301d3053bbc0ab7baea4ea92b440add63734369/src/utils.hpp#L40"><code class="language-plaintext highlighter-rouge">init_kernels</code></a> loads the OpenCL kernels from a separate binary file and stores references to them in global variables.
Next, we load the model weights from separate <code class="language-plaintext highlighter-rouge">.npy</code> files, one for each tensor.
Finally, we also load the input samples that we will feed into the model.
These <code class="language-plaintext highlighter-rouge">.npy</code> files are prepared by the <a href="https://github.com/dsuess/nn-on-fpgas/blob/f301d3053bbc0ab7baea4ea92b440add63734369/train.py#L97">train script</a> and stored as float32 arrays.</p>

<p>Next, we run the model, wait for all OpenCL events to finish, and copy the result back from the device to the host:</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">auto</span> <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>

    <span class="n">finish_cl_queue</span><span class="p">();</span>
    <span class="n">result</span><span class="p">.</span><span class="n">to_cpu</span><span class="p">();</span>
    <span class="n">finish_cl_queue</span><span class="p">();</span>
</code></pre></div></div>
<p>Finally, for each element in the batch, we compute the argmax of the confidence scores to get the final prediction and print it to stdout:</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// print argmax result</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">result</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="kt">float</span> <span class="n">minval</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">result</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">auto</span> <span class="n">val</span> <span class="o">=</span> <span class="n">result</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">minval</span> <span class="o">&lt;</span> <span class="n">val</span><span class="p">)</span>
            <span class="p">{</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
                <span class="n">minval</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">idx</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="err">}</span>
</code></pre></div></div>

<p>In the next posts we will look at the performance characteristics of this baseline implementation and how to improve both latency as well as reduce the resource requirements.</p>

<blockquote>
  <p><em>Cover Image</em> by <a href="https://commons.wikimedia.org/wiki/File:Xilinx_FPGA_XCV400E-PQ240.jpg">Pedant01</a> CC BY-SA 4.0</p>
</blockquote>

  </div><a class="u-url" href="/blog/2021-05-10-fpga-part-2/" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/dsuess" target="_blank" title="dsuess"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/daniel-suess" target="_blank" title="daniel-suess"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/suess_daniel" target="_blank" title="suess_daniel"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
